{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 250/250 [01:41<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, average loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 250/250 [01:41<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, average loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 250/250 [01:41<00:00,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, average loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Uses the last 4 videos (5-8) of https://www.youtube.com/playlist?list=PLQVvvaa0QuDdeMyHEYc0gxFpYwHY2Qfdh\n",
    "# Read notes/nn_notes.md\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import functional as nn_func\n",
    "from torch import optim\n",
    "\n",
    "\n",
    "REBUILD_DATA = False  # Don't want to rerun data preprocessing each time program runs\n",
    "\n",
    "print(\"starting!\")\n",
    "class DogsAndCatsData:\n",
    "    # I have no idea why the tutorial didn't use Dataset and DataLoader but whatever\n",
    "    IMG_SIZE = (50, 50)  # resize images, raw input images are in diff sizes & ratios\n",
    "    CATS = \"data/CatsAndDogs/PetImages/Cat\"\n",
    "    DOGS = \"data/CatsAndDogs/PetImages/Dog\"\n",
    "    LABELS = {CATS: 0, DOGS: 1}\n",
    "\n",
    "    training_data = []\n",
    "    cat_count = 0  # balancing\n",
    "    dog_count = 0\n",
    "\n",
    "    def build_training_data(self):\n",
    "        for label in self.LABELS.keys():\n",
    "            print(f\"CREATING DATA FROM: {label}\")\n",
    "            for file in tqdm(os.listdir(label)):  # tqdm just wraps an iterable in a progress bar\n",
    "                path = os.path.join(label, file)\n",
    "                # read & convert to grayscale, color is added data that is not needed\n",
    "                img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "                if img is None:  # bad images\n",
    "                    continue\n",
    "                img = cv2.resize(img, self.IMG_SIZE)\n",
    "                # np.eye() used to create one-hot vectors so that cat=[1,0] & dog=[0,1]. eye(n) outputs a n*n array with\n",
    "                # a diagonal across it such that array[n][n] = 0. (Ex: eye(3) -> [[1,0,0][0,1,0][0,0,1]])\n",
    "                self.training_data.append((img, np.eye(2)[self.LABELS[label]]))\n",
    "\n",
    "                if label == self.DOGS:\n",
    "                    self.dog_count += 1\n",
    "                elif label == self.CATS:\n",
    "                    self.cat_count += 1\n",
    "        print(f\"{self.cat_count} cats, {self.dog_count} dogs\")\n",
    "\n",
    "        np.random.shuffle(self.training_data)\n",
    "        np.save(\"training_data.npy\", self.training_data)  # Save training data for later use\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, (5, 5))\n",
    "        self.conv2 = nn.Conv2d(32, 64, (5, 5))\n",
    "        self.conv3 = nn.Conv2d(64, 128, (5, 5))\n",
    "\n",
    "        \"\"\"\n",
    "        Convolutional layers output differently from linear ones, outputting a tensor of n*n*out_channels.\n",
    "        Thus, they need to be flattened. However, this varies based on input size, so just shove in a 50x50 of zeros,\n",
    "        get the output shape, flatten, and use that.\n",
    "        - In this case, the output size (from a 50*50 input) is 512 (2*2*128)\n",
    "        \n",
    "        - This reshapes into a 4d array because the input format is:\n",
    "         [\n",
    "            [\n",
    "                [[data], \n",
    "                [data]],\n",
    "                 \n",
    "                [[data_2],\n",
    "                 [data_2]], \n",
    "            ], \n",
    "            [label, label_2]\n",
    "        ]\n",
    "        \"\"\"\n",
    "        self.conv_output_flattened_size = self.run_conv_layers(torch.zeros(50, 50).view(-1, 1, 50, 50)).flatten().size()[0]\n",
    "        self.zero_grad()\n",
    "\n",
    "        self.fc1 = nn.Linear(self.conv_output_flattened_size, 512)\n",
    "        self.fc2 = nn.Linear(512, 2)  # output layer\n",
    "\n",
    "    def run_conv_layers(self, input_data):\n",
    "        # (2, 2) is the max pool kernel shape\n",
    "        features = nn_func.max_pool2d(nn_func.relu(self.conv1(input_data)), (2, 2))\n",
    "        features = nn_func.max_pool2d(nn_func.relu(self.conv2(features)), (2, 2))\n",
    "        features = nn_func.max_pool2d(nn_func.relu(self.conv3(features)), (2, 2))\n",
    "\n",
    "        return features\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        conv_features = self.run_conv_layers(input_data)\n",
    "        # flatten into a tensor of tensors of flattened features\n",
    "        conv_features = conv_features.view(-1, self.conv_output_flattened_size)\n",
    "        activations = nn_func.relu(self.fc1(conv_features))\n",
    "        output = self.fc2(activations)\n",
    "        return nn_func.softmax(output, dim=1)\n",
    "\n",
    "\n",
    "if REBUILD_DATA:\n",
    "    DogsAndCatsData().build_training_data()\n",
    "\n",
    "train_data = np.load(\"training_data.npy\", allow_pickle=True)\n",
    "\n",
    "images = Tensor(np.array([example[0] for example in train_data])).view(-1, 50, 50)\n",
    "images = images/255  # make grayscale values a percentage of 255 so that 0 < x < 1\n",
    "labels = Tensor(np.array([example[1] for example in train_data]))\n",
    "\n",
    "TEST_PERCENT = 0.10  # Use 10% of the data as tests\n",
    "\n",
    "train_images = images[:-int((len(images)*TEST_PERCENT))]\n",
    "train_labels = labels[:-int((len(images)*TEST_PERCENT))]\n",
    "test_images = images[-int((len(images)*TEST_PERCENT)):]\n",
    "test_labels = labels[-int((len(images)*TEST_PERCENT)):]\n",
    "\n",
    "network = Network()\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.001)\n",
    "\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "\n",
    "# Training:\n",
    "network.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    # I have no idea why the tutorial didn't use Dataset and DataLoader but whatever\n",
    "    total_loss = []\n",
    "    for index in tqdm(range(0, len(train_data), BATCH_SIZE)):\n",
    "        batch_images = train_images[index:index+BATCH_SIZE].view(-1, 1, 50, 50)\n",
    "        batch_labels = train_labels[index:index+BATCH_SIZE]\n",
    "\n",
    "        # optimizer.zero_grad() does the same thing IF optimizer was given all parameters (network.parameters())\n",
    "        network.zero_grad()\n",
    "        outputs = network(batch_images)\n",
    "        loss = nn.MSELoss()(outputs, batch_labels)  # MSELoss is a callable class\n",
    "        loss.backward()\n",
    "        total_loss.append(loss.item())\n",
    "        optimizer.step()\n",
    "    print(f\"epoch {epoch}, average loss: {sum(total_loss)/len(total_loss)}\")\n",
    "\n",
    "\n",
    "# print(Network().run_conv_layers(torch.zeros(50, 50).view(-1, 1, 50, 50)).flatten().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 2494/2494 [00:05<00:00, 430.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 1539, total: 2494 | 61.708099438652766%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    network.eval()\n",
    "    for index in tqdm(range(len(test_images))):\n",
    "        answer = torch.argmax(test_labels[index])  # argmax returns the INDEX of largest element\n",
    "        prediction = torch.argmax(network(test_images[index].view(-1, 1, 50, 50)))\n",
    "#         print(test_labels[index], network(test_images[index].view(-1, 1, 50, 50)))\n",
    "        if answer == prediction:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "        \n",
    "print(f\"Correct: {correct}, total: {total} | {(correct/total)*100}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[array([[ 66,  51,  49, ...,  59,  11,  12],\n",
      "         [ 49,  47,  53, ...,  49,  35,  11],\n",
      "         [ 49,  45,  52, ...,  41,  19,  13],\n",
      "         ...,\n",
      "         [133, 137, 141, ..., 135, 131, 133],\n",
      "         [137, 140, 125, ..., 146, 142, 141],\n",
      "         [131, 140, 135, ..., 146, 139, 151]], dtype=uint8)\n",
      "  array([0., 1.])]\n",
      " [array([[131, 158, 182, ..., 195, 193, 196],\n",
      "         [140, 132, 206, ..., 198, 197, 196],\n",
      "         [120, 116, 199, ..., 195, 199, 199],\n",
      "         ...,\n",
      "         [177, 144, 157, ...,  80, 110, 159],\n",
      "         [125, 140, 156, ...,  76, 156, 157],\n",
      "         [141, 127, 153, ...,  80, 155, 157]], dtype=uint8)\n",
      "  array([1., 0.])]\n",
      " [array([[ 84,  77,  91, ..., 150, 100, 167],\n",
      "         [ 93,  93, 102, ..., 100, 119,  81],\n",
      "         [114, 113, 117, ...,  73, 152, 200],\n",
      "         ...,\n",
      "         [231, 225, 241, ..., 140, 151, 147],\n",
      "         [239, 225, 245, ..., 147, 139, 123],\n",
      "         [236, 232, 243, ..., 134, 145, 139]], dtype=uint8)\n",
      "  array([1., 0.])]\n",
      " ...\n",
      " [array([[199, 199, 201, ..., 183, 181, 179],\n",
      "         [199, 199, 201, ..., 183, 181, 179],\n",
      "         [199, 200, 201, ..., 183, 181, 180],\n",
      "         ...,\n",
      "         [192, 193, 193, ..., 177, 176, 177],\n",
      "         [191, 192, 193, ..., 176, 174, 175],\n",
      "         [137, 135, 137, ..., 181, 162, 162]], dtype=uint8)\n",
      "  array([1., 0.])]\n",
      " [array([[ 49,  98,  97, ...,  57, 106, 107],\n",
      "         [ 68, 102,  84, ..., 102,  83,  30],\n",
      "         [ 66, 104,  43, ...,  38, 111, 104],\n",
      "         ...,\n",
      "         [ 73,  24,  85, ...,   9,   4,   6],\n",
      "         [ 67,  27,  84, ...,   6,   3,   6],\n",
      "         [ 63,  36,  78, ...,   6,   2,   4]], dtype=uint8)\n",
      "  array([1., 0.])]\n",
      " [array([[ 43,  63,  77, ..., 163, 152, 136],\n",
      "         [ 30,  32,  34, ..., 164, 135, 128],\n",
      "         [ 34,  34,  31, ..., 149, 163, 113],\n",
      "         ...,\n",
      "         [ 33,  38,  63, ...,  75,  71,  83],\n",
      "         [ 30,  25,  59, ...,  85,  86,  69],\n",
      "         [ 40,  22,  31, ...,  75,  80,  63]], dtype=uint8)\n",
      "  array([1., 0.])]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9648/4240448462.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mtrain_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# from 0, to the len of x, stepping BATCH_SIZE at a time. [:50] ..for now just to dev\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
